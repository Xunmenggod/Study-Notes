{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principle and theory and implementation of reinforcement learning\n",
    "\n",
    "## Algorithm type\n",
    "- On-policy algorithm\n",
    "\n",
    "- Offline-policy alorithm\n",
    "\n",
    "## Data collection maybe online or offline\n",
    "- Rollout buffer: n_steps rollout data for all environments\n",
    "  rollout data should include action taken, obs_new, done_flag, infos, critic values\n",
    "  when the rollout data buffer is full which means the data buffer is ready to update the policy. Before trainning, the rollout data will be used to calculate the return and advatanges based on rewards. The value from the critic network need to be stored for reurns and advantages calculation\n",
    "  `Equation of cal of returns and advantages`: advantages = diff(between one step discounted rewards and predicted critic values) + gamma * gae_lambda * next_non_terminal * last_advantages, returns = advantages + values\n",
    "- Trainning policy for the collected rollout data (policy update)\n",
    "  In each epoch, go through the whole rollout data batch by batch and update the weight of policy based on the loss, where loss = policy_loss + self.ent_coef * entropy_loss + self.vf_coef * value_loss\n",
    "\n",
    "\n",
    "## Basic theory\n",
    "\n",
    "\n",
    "## Specific algorithm idea and novelty\n",
    "\n",
    "\n",
    "\n",
    "## Implementation benchmarks trick\n",
    "- stable baseline3\n",
    "- rl games\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
