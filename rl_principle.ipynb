{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principle and theory and implementation of reinforcement learning\n",
    "\n",
    "## Algorithm type\n",
    "- On-policy algorithm\n",
    "\n",
    "- Offline-policy alorithm\n",
    "\n",
    "## Data collection maybe online or offline\n",
    "- Rollout buffer: n_steps rollout data for all environments\n",
    "  rollout data should include action taken, obs_new, done_flag, infos, critic values\n",
    "  when the rollout data buffer is full which means the data buffer is ready to update the policy. Before trainning, the rollout data will be used to calculate the return and advatanges based on rewards. The value from the critic network need to be stored for reurns and advantages calculation\n",
    "  `Equation of cal of returns and advantages`: advantages = diff(between one step discounted rewards and predicted critic values) + gamma * gae_lambda * next_non_terminal * last_advantages, returns = advantages + values\n",
    "- Trainning policy for the collected rollout data (policy update)\n",
    "  In each epoch, go through the whole rollout data batch by batch and update the weight of policy based on the loss, where loss = policy_loss + self.ent_coef * entropy_loss + self.vf_coef * value_loss\n",
    "\n",
    "\n",
    "## Basic theory\n",
    "- Policy gradient method\n",
    "  policy graident loss = $\\frac{1}{m}\\sum_{i=1}^{m} \\nabla \\log P(\\tau_i;\\theta)(R(\\tau_i) - V_\\phi)$, actually advatange is exactly the difference between the trajectory reward and the estimated value from critic function. We also tried to minimize the advantage to update the critic parameter.\n",
    "- Pseudo-code for policy gradient rl\n",
    "  1. Initialize the parameter for both actor and critic network\n",
    "  2. for i in number of epoches \n",
    "      collect m trajectories based on current policy\n",
    "      At each timestep in each trajectory, compute the return $R_t = \\sum_{t^=t}^{T-1} \\gamma^{t'-t}r_{t'}$ and advantage $\\hat{A} = R_t - V(s_t)$\n",
    "      Update critic network by MSE loss between predicted value and real return across all\n",
    "      timesteps of all trajectories\n",
    "      Update the actor network which is also the policy iteself, we maximize the policy reward by calculating policy gradient loss\n",
    "      end for loop\n",
    "\n",
    "## Specific algorithm idea and novelty\n",
    "\n",
    "\n",
    "\n",
    "## Implementation benchmarks trick\n",
    "- stable baseline3\n",
    "- rl games\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
